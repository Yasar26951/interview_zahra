# Zahra â€“ AI Interview Preparation Assistant ğŸ¤–ğŸ¯

Zahra is an AI-powered interview preparation assistant designed to help students answer interview questions **clearly, professionally, and confidently**.  
The system is built using a **fine-tuned Large Language Model (LLM)** with **QLoRA**, orchestrated through **LangGraph** for structured conversational flow.

---

## ğŸš€ Features

- ğŸ’¬ Provides clear and professional answers to interview questions  
- ğŸ§  Fine-tuned LLM using **QLoRA** for efficient training on limited compute  
- ğŸ” Structured conversation handling using **LangGraph**  
- ğŸ¯ Focused on interview concepts, HR questions, and behavioral answers  
- âš¡ Optimized inference using Hugging Face pipelines  

---

## ğŸ—ï¸ Architecture Overview

- **Base Model:** Qwen2.5-3B-Instruct  
- **Fine-tuning:** QLoRA (PEFT)  
- **Prompting Style:** Instructionâ€“Contextâ€“Response format  
- **Orchestration:** LangGraph (state-based execution)  
- **Inference:** Hugging Face `pipeline` + LangChain wrapper  

---

## ğŸ“ Project Structure
```
Zahra_interview/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ agent.py # LangGraph logic and response generation
â”‚ â”œâ”€â”€ app.py # Backend entry point (API / server logic)
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ index.html # Chat UI
â”‚ â”œâ”€â”€ script.js # Frontend interaction logic
â”‚ â””â”€â”€ style.css # Styling
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  Prompt Template Used

The model was fine-tuned and invoked using the following structured format:

Instruction:
You are a helpful interview assistant called Zahra.
Help the student answer interview questions professionally and clearly.

Context:
<User Question>
Response:
<Model Answer> ```
This ensures consistent, role-aligned, and concise responses.

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
``` bash
git clone https://github.com/Yasar26951/interview_zahra.git
cd Zahra_interview
```
2ï¸âƒ£ Create Virtual Environment
``` bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```
3ï¸âƒ£ Install Dependencies
``` bash
pip install -r requirement.txt
```
â–¶ï¸ Running the Application
Backend
```bash
python backend/app.py
```
Frontend
Open frontend/index.html in a browser.

## you can play with my model using huggingface 

``` python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

from transformers import pipeline
base_model = "Qwen/Qwen2.5-3B-Instruct"
adapter_repo = "Mohamed26/Qwen2.5-3B-Instruct-qlora-zahra"

tokenizer = AutoTokenizer.from_pretrained(base_model)
base = AutoModelForCausalLM.from_pretrained(base_model, device_map="auto")

model = PeftModel.from_pretrained(base, adapter_repo)
gen_pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True,
)
##now play with the model
```



